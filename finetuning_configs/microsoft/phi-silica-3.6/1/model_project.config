{
    "workflows": [
        {
            "name": "LoRA",
            "description": "Adapts large models with small trainable layers, reducing memory and compute needs without changing model weights.",
            "file": "lora/infra/provision/finetuning.config.json",
            "template": "finetuning_configs/microsoft/phi-silica-3.6",
            "version": 1,
            "templateName": "lora"
        }
    ],
    "modelInfo": {
        "id": "microsoft/phi-silica-3.6",
        "type": "finetuning",
        "version": 1
    }
}
