[
    {
        "groupId": "data",
        "fields": [
            {
                "type": "String",
                "label": "Training Dataset name:",
                "info": "Dataset to train the model from a local file.",
                "defaultValue": "",
                "fileName": "train.json",
                "sample": "{\"messages\":[{\"content\":\"Hello, bot.\",\"role\":\"user\"},{\"content\":\"Hi! How can I assist?\",\"role\":\"assistant\"}]}\n{\"messages\":[{\"content\":\"Who are you?\",\"role\":\"user\"},{\"content\":\"I am your assistant.\",\"role\":\"assistant\"}]}"
            },
            {
                "type": "String",
                "label": "Test Dataset name:",
                "info": "Dataset to evaluate the model from a local file.",
                "defaultValue": "",
                "fileName": "test.json",
                "sample": "{\"messages\":[{\"content\":\"Hello, bot.\",\"role\":\"user\"},{\"content\":\"Hi! How can I assist?\",\"role\":\"assistant\"}]}\n{\"messages\":[{\"content\":\"Who are you?\",\"role\":\"user\"},{\"content\":\"I am your assistant.\",\"role\":\"assistant\"}]}"
            },
            {
                "type": "Integer",
                "defaultValue": 8000,
                "info": "Number of samples to load from the train set for finetuning.",
                "label": "Number of finetuning samples:",
                "id": "finetune_train_nsamples"
            },
            {
                "type": "Integer",
                "defaultValue": 128,
                "info": "Number of samples to load from the test set for finetuning.",
                "label": "Number of test samples:",
                "id": "finetune_test_nsamples"
            },
            {
                "type": "Integer",
                "defaultValue": 2,
                "info": "Batch size for finetuning training.",
                "label": "Training batch size:",
                "id": "finetune_train_batch_size"
            },
            {
                "type": "Integer",
                "defaultValue": 8,
                "info": "Batch size for finetuning testing.",
                "label": "Test batch size:",
                "id": "finetune_test_batch_size"
            },
            {
                "type": "Integer",
                "defaultValue": 1024,
                "info": "Maximum sequence length for finetuning training. Longer sequences will be truncated.",
                "label": "Max training sequence length:",
                "replaceToken": "finetune_train_seqlen"
            },
            {
                "type": "Integer",
                "defaultValue": 2048,
                "info": "Maximum sequence length for finetuning testing. Longer sequences will be truncated.",
                "label": "Max test sequence length:",
                "replaceToken": "finetune_test_seqlen"
            }
        ],
        "title": "Data"
    },
    {
        "groupId": "finetuning",
        "fields": [
            {
                "type": "Number",
                "defaultValue": 0.1,
                "info": "Dropout rate for LoRA.",
                "label": "LoRA dropout:",
                "id": "lora_dropout",
                "min": 0,
                "max": 0.5
            },
            {
                "type": "Number",
                "defaultValue": 0.0002,
                "info": "Learning rate for training.",
                "label": "Learning rate:",
                "id": "learning_rate",
                "min": 1e-4,
                "max": 1e-2
            },
            {
                "type": "Number",
                "defaultValue": 0.9,
                "info": "Beta1 hyperparameter for Adam optimizer.",
                "label": "Adam beta 1:",
                "id": "adam_beta1",
                "min": 0.9,
                "max": 0.99
            },
            {
                "type": "Number",
                "defaultValue": 0.95,
                "info": "Beta2 hyperparameter for Adam optimizer.",
                "label": "Adam beta 2:",
                "id": "adam_beta2",
                "min": 0.9,
                "max": 0.999
            },
            {
                "type": "Number",
                "defaultValue": 1e-8,
                "info": "Epsilon hyperparameter for Adam optimizer.",
                "label": "Adam epsilon:",
                "id": "adam_epsilon",
                "min": 1e-9,
                "max": 1e-6
            },
            {
                "type": "String",
                "defaultValue": "linear_with_warmup",
                "info": "Type of learning rate scheduler.",
                "label": "Learning rate scheduler:",
                "optionValues": [
                    "linear_with_warmup",
                    "cosine_with_warmup"
                ],
                "id": "lr_scheduler_type"
            },
            {
                "type": "Integer",
                "defaultValue": 400,
                "info": "Number of warmup steps for learning rate scheduler. Only relevant for a _with_warmup scheduler.",
                "label": "Scheduler warmup steps (if supported):",
                "id": "num_warmup_steps",
                "min": 0,
                "max": 10000
            }
        ],
        "title": "Finetuning"
    }
]