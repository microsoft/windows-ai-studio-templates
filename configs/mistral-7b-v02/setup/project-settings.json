[
  {
    "groupId": "model_inference",
    "title": "Model inference settings",
    "fields": [
      {
        "type": "String",
        "label": "Conda environment name:",
        "info": "Name of the conda environment to activate in WSL.",
        "replaceToken": "<conda_env_name>",
        "defaultValue": "mistral-7b-v02-env"
      },
      {
        "type": "String",
        "label": "Inference prompt template:",
        "info": "Prompt template used at inference time, make sure it matches the finetuned version.",
        "replaceToken": "<prompt_template>",
        "defaultValue": "### Text: {}\\n### The tone is:\\n"
      }
    ]
  },
  {
    "groupId": "data_configs",
    "title": "Data settings",
    "fields": [
      {
        "type": "String",
        "label": "Dataset name:",
        "info":"Dataset to train the model from a local file.",
        "replaceToken": "<data_configs_data_files>",
        "optionValues": ["dataset/dataset-classification.json"],
        "defaultValue": "dataset/dataset-classification.json"
      },
      {
        "type": "String",
        "label": "Training split:",
        "info": "Training split name for your dataset.",
        "replaceToken": "<data_configs_split>",
        "defaultValue": "train"
      },
      {
        "type": "String",
        "label": "Dataset type:",
        "replaceToken": "<dataset_type>",
        "defaultValue": "corpus"
      },
      {
        "type": "String",
        "isArray": true,
        "label": "Text columns:",
        "info": "Columns that match your dataset to populate the training prompt.",
        "replaceToken": "<text_cols>",
        "defaultValue": [ "phrase", "tone" ]
      },
      {
        "type": "String",
        "label": "Text template:",
        "info": "Prompt template to finetune the model, it uses replacement from with the columns.",
        "replaceToken": "<text_template>",
        "defaultValue": "### Text: {phrase}\\n### The tone is:\\n{tone}"
      },
      {
        "type": "String",
        "label": "Corpus strategy:",
        "info": "Do you want to join the samples or process them one by one.",
        "replaceToken": "<line_by_line>",
        "defaultValue": "join",
        "optionValues": ["line-by-line", "join"]
      },
      {
        "type": "Integer",
        "label": "Source max length:",
        "info": "Max numbers of tokens per traning sample.",
        "replaceToken": "<source_max_len>",
        "defaultValue": 1024
      },     
      {
        "type": "Boolean",
        "label": "Pad to max length:",
        "info": "Add PAD token to the training sample until the max number of tokens.",
        "replaceToken": "<pad_to_max_len>",
        "defaultValue": false
      }
    ]
  },
  {
    "groupId": "fine_tune",
    "title": "Fine tune settings",
    "fields": [
      {
        "type": "String",
        "label": "Compute dtype:",
        "info": "Data type for model weights and adapter weights.",
        "replaceToken": "<compute_dtype>",
        "optionValues": ["bfloat16", "float16"],
        "defaultValue": "bfloat16"
      },
      {
        "type": "String",
        "label": "Quant type:",
        "info": "Quantization data type to use. Should be one of fp4 or nf4.",
        "replaceToken": "<quant_type>",
        "optionValues": ["nf4", "fp4"],
        "defaultValue": "nf4"
      },
      {
        "type": "Boolean",
        "label": "Double quant:",
        "info": "Whether to use nested quantization where the quantization constants from the first quantization are quantized again.",
        "replaceToken": "<double_quant>",
        "defaultValue": true
      },
      {
        "type": "Integer",
        "label": "Lora r:",
        "info": "Lora attention dimension.",
        "replaceToken": "<lora_r>",
        "defaultValue": 32
      },
      {
        "type": "Integer",
        "label": "Lora alpha:",
        "info": "The alpha parameter for Lora scaling",
        "replaceToken": "<lora_alpha>",
        "defaultValue": 64
      },
      {
        "type": "Number",
        "label": "Lora dropout:",
        "info": "The dropout probability for Lora layers",
        "replaceToken": "<lora_dropout>",
        "defaultValue": 0.1
      },
      {
        "type": "Integer",
        "label": "Eval dataset size:",
        "info": "Size of the validation dataset, a number or 0-1 percentage.",
        "replaceToken": "<eval_dataset_size>",
        "defaultValue": 0.3
      },    
      {
        "type": "Integer",
        "label": "Seed:",
        "info": "Random seed for initialization.",
        "replaceToken": "<training_args_seed>",
        "defaultValue": 0
      },
      {
        "type": "Integer",
        "label": "Data seed:",
        "info": "Random seed to be used with data samplers.",
        "replaceToken": "<training_args_data_seed>",
        "defaultValue": 42
      },
      {
        "type": "Integer",
        "label": "Per device train batch size:",
        "info": "The batch size per GPU for training.",
        "replaceToken": "<per_device_train_batch_size>",
        "defaultValue": 8
      },
      {
        "type": "Integer",
        "label": "Per device eval batch size:",
        "info": "The batch size per GPU for evaluation.",
        "replaceToken": "<per_device_eval_batch_size>",
        "defaultValue": 8
      },
      {
        "type": "Integer",
        "label": "Gradient accumulation steps:",
        "info": "Number of updates steps to accumulate the gradients for, before performing a backward/update pass",
        "replaceToken": "<gradient_accumulation_steps>",
        "defaultValue": 4
      },
      {
        "type": "Boolean",
        "label": "Enable gradient checkpointing:",
        "info": "Use gradient checkpointing. Recommended to save the memory.",
        "replaceToken": "<gradient_checkpointing>",
        "defaultValue": true
      },
      {
        "type": "Number",
        "label": "Learning rate:",
        "info": "The initial learning rate for AdamW",
        "replaceToken": "<learning_rate>",
        "defaultValue": 0.0002
      },
      {
        "type": "Integer",
        "label": "Number of epochs:",
        "info": "How many complete passes the model will make over the entire training dataset.",
        "replaceToken": "<num_train_epochs>",
        "defaultValue": 3
      },
      {
        "type": "Integer",
        "label": "Max steps:",
        "info":"Training will be stopped when this number of steps is reached, regardless of the number of epochs.",
        "replaceToken": "<max_steps>",
        "defaultValue": 80 
      },
      {
        "type": "String",
        "label": "Checkpoint output dir",
        "info": "Directory to save the checkpoints.",
        "replaceToken": "<output_dir>",
        "defaultValue": "models/checkpoints"
      }
    ]
  }    
]
