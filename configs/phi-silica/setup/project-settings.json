[
    {
        "groupId": "data",
        "fields": [
            {
                "type": "Integer",
                "defaultValue": 42,
                "info": "Seed for sampling the data.",
                "label": "Random seed:",
                "replaceToken": "<training_args_seed>"
            },
            {
              "type": "String",
              "label": "Training Dataset name:",
              "info": "Dataset to train the model from a local file.",
              "replaceToken": "<data_configs_data_files>",
              "optionValues": [
                "dataset/train.json"
              ],
              "defaultValue": "dataset/train.json",
              "fileName": "train.json"
            },
            {
              "type": "String",
              "label": "Test Dataset name:",
              "info": "Dataset to evaluate the model from a local file.",
              "replaceToken": "<data_configs_data_files>",
              "optionValues": [
                "dataset/test.json"
              ],
              "defaultValue": "dataset/test.json",
              "fileName": "test.json"
            },
            {
                "type": "Integer",
                "defaultValue": 8000,
                "info": "Number of samples to load from the train set for finetuning.",
                "label": "Number of finetuning samples:",
                "replaceToken": "<finetune_train_nsamples>"
            },
            {
                "type": "Integer",
                "defaultValue": 128,
                "info": "Number of samples to load from the test set for finetuning.",
                "label": "Number of test samples:",
                "replaceToken": "<finetune_test_nsamples>"
            },
            {
                "type": "Integer",
                "defaultValue": 2,
                "info": "Batch size for finetuning training.",
                "label": "Training batch size:",
                "replaceToken": "<finetune_train_batch_size>"
            },
            {
                "type": "Integer",
                "defaultValue": 8,
                "info": "Batch size for finetuning testing.",
                "label": "Test batch size:",
                "replaceToken": "<finetune_test_batch_size>"
            },
            {
                "type": "Integer",
                "defaultValue": 1024,
                "info": "Maximum sequence length for finetuning training. Longer sequences will be truncated.",
                "label": "Max training sequence length:",
                "replaceToken": "<finetune_train_seqlen>"
            },
            {
                "type": "Integer",
                "defaultValue": 2048,
                "info": "Maximum sequence length for finetuning testing. Longer sequences will be truncated.",
                "label": "Max test sequence length:",
                "replaceToken": "<finetune_test_seqlen>"
            }
        ],
        "title": "Data"
    },
    {
        "groupId": "finetuning",
        "fields": [
            {
                "type": "Integer",
                "defaultValue": 5,
                "info": "Number of evaluations with no improvement after which training will be stopped.",
                "label": "Early stopping patience:",
                "replaceToken": "<early_stopping_patience>"
            },
            {
                "type": "Number",
                "defaultValue": 1,
                "info": "Number of total epochs to run.",
                "label": "Epochs:",
                "replaceToken": "<training_args_epochs>"
            },
            {
                "type": "String",
                "defaultValue": "steps",
                "info": "Evaluation strategy to use.",
                "label": "Evaluation strategy:",
                "optionValues": [
                    "steps",
                    "epoch",
                    "no"
                ],
                "replaceToken": "<evaluation_strategy>"
            },
            {
                "type": "Integer",
                "defaultValue": 64,
                "info": "Number of training steps to perform before each evaluation.",
                "label": "Steps between evaluations:",
                "replaceToken": "<eval_steps>"
            },
            {
                "type": "Integer",
                "defaultValue": 64,
                "info": "Number of steps after which to save model checkpoint. This _must_ be a multiple of the number of steps between evaluations.",
                "label": "Steps between checkpoints:",
                "replaceToken": "<save_steps>"
            },
            {
                "type": "Number",
                "defaultValue": 0.1,
                "info": "Dropout rate for LoRA.",
                "label": "LoRA dropout:",
                "replaceToken": "<lora_dropout>"
            },
            {
                "type": "Number",
                "defaultValue": 0.0002,
                "info": "Learning rate for training.",
                "label": "Learning rate:",
                "replaceToken": "<learning_rate>"
            },
            {
                "type": "Number",
                "defaultValue": 0.9,
                "info": "Beta1 hyperparameter for Adam optimizer.",
                "label": "Adam beta 1:",
                "replaceToken": "<adam_beta1>"
            },
            {
                "type": "Number",
                "defaultValue": 0.95,
                "info": "Beta2 hyperparameter for Adam optimizer.",
                "label": "Adam beta 2:",
                "replaceToken": "<adam_beta2>"
            },
            {
                "type": "Number",
                "defaultValue": 1e-08,
                "info": "Epsilon hyperparameter for Adam optimizer.",
                "label": "Adam epsilon:",
                "replaceToken": "<adam_epsilon>"
            },
            {
                "type": "String",
                "defaultValue": "linear",
                "info": "Type of learning rate scheduler.",
                "label": "Learning rate scheduler:",
                "optionValues": [
                    "linear",
                    "linear_with_warmup",
                    "cosine",
                    "cosine_with_warmup"
                ],
                "replaceToken": "<lr_scheduler_type>"
            },
            {
                "type": "Integer",
                "defaultValue": 400,
                "info": "Number of warmup steps for learning rate scheduler. Only relevant for a _with_warmup scheduler.",
                "label": "Scheduler warmup steps (if supported):",
                "replaceToken": "<num_warmup_steps>"
            },
            {
                "type": "Number",
                "defaultValue": null,
                "info": "The number of training steps there will be. If not set (recommended), this will be calculated internally.",
                "label": "Number of training steps:",
                "replaceToken": "<num_training_steps>"
            }
        ],
        "title": "Finetuning"
    },
    {
        "groupId": "advanced",
        "fields": [
            {
                "type": "Integer",
                "defaultValue": 1,
                "info": "Number of updates steps to accumulate before performing a backward/update pass.",
                "label": "gradient accumulation steps:",
                "replaceToken": "<gradient_accumulation_steps>"
            },
            {
                "type": "Number",
                "defaultValue": null,
                "info": "Number of predictions steps to accumulate before moving the tensors to the CPU.",
                "label": "eval accumulation steps:",
                "replaceToken": "<eval_accumulation_steps>"
            },
            {
                "type": "Number",
                "defaultValue": 0,
                "info": "Number of epochs or steps to wait for before the first evaluation can be performed, depending on the eval_strategy.",
                "label": "eval delay:",
                "replaceToken": "<eval_delay>"
            },
            {
                "type": "Number",
                "defaultValue": 0.0,
                "info": "Weight decay for AdamW if we apply some.",
                "label": "weight decay:",
                "replaceToken": "<weight_decay>"
            },
            {
                "type": "Number",
                "defaultValue": 1.0,
                "info": "Max gradient norm.",
                "label": "max grad norm:",
                "replaceToken": "<max_grad_norm>"
            },
            {
                "type": "String",
                "defaultValue": false,
                "info": "If True, use gradient checkpointing to save memory at the expense of slower backward pass.",
                "label": "gradient checkpointing:",
                "replaceToken": "<gradient_checkpointing>"
            }
        ],
        "title": "Advanced",
        "collapsible": true,
        "collapsed": true
    }
]