adam_beta1: <adam_beta1>
adam_beta2: <adam_beta2>
adam_epsilon: !!float <adam_epsilon>
early_stopping_patience: <early_stopping_patience>
epochs: <training_args_epochs>
eval_accumulation_steps: <eval_accumulation_steps>
eval_delay: <eval_delay>
eval_steps: <eval_steps>
evaluation_strategy: <evaluation_strategy>
finetune_dataset: mount/<run_id>/dataset
finetune_test_batch_size: <finetune_test_batch_size>
finetune_test_nsamples: <finetune_test_nsamples>
finetune_test_seqlen: <finetune_test_seqlen>
finetune_train_batch_size: <finetune_train_batch_size>
finetune_train_nsamples: <finetune_train_nsamples>
finetune_train_seqlen: <finetune_train_seqlen>
gradient_accumulation_steps: <gradient_accumulation_steps>
gradient_checkpointing: <gradient_checkpointing>
learning_rate: <learning_rate>
lora_dropout: <lora_dropout>
lr_scheduler_type: <lr_scheduler_type>
max_grad_norm: <max_grad_norm>
model: Phi-3.6-mini-instruct
model_path: model
num_training_steps: <num_training_steps>
num_warmup_steps: <num_warmup_steps>
save_steps: <save_steps>
seed: <training_args_seed>
weight_decay: <weight_decay>
