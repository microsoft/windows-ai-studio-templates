{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Who is Isaac Newton?'\n",
    "ExecutionProvider=\"QNNExecutionProvider\"\n",
    "model_folder = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_winrt_runtime():\n",
    "    \"\"\"This function removes the msvcp140.dll from the winrt-runtime package.\n",
    "    So it does not cause issues with other libraries.\n",
    "    \"\"\"\n",
    "    from importlib import metadata\n",
    "    from pathlib import Path\n",
    "    site_packages_path = Path(str(metadata.distribution('winrt-runtime').locate_file('')))\n",
    "    dll_path = site_packages_path / 'winrt' / 'msvcp140.dll'\n",
    "    if dll_path.exists():\n",
    "        dll_path.unlink()\n",
    "            \n",
    "def _get_ep_paths() -> dict[str, str]:\n",
    "    from winui3.microsoft.windows.applicationmodel.dynamicdependency.bootstrap import (\n",
    "        InitializeOptions,\n",
    "        initialize\n",
    "    )\n",
    "    import winui3.microsoft.windows.ai.machinelearning as winml\n",
    "    eps = {}\n",
    "    with initialize(options = InitializeOptions.ON_NO_MATCH_SHOW_UI):\n",
    "        catalog = winml.ExecutionProviderCatalog.get_default()\n",
    "        providers = catalog.find_all_providers()\n",
    "        for provider in providers:\n",
    "            provider.ensure_ready_async().get()\n",
    "            eps[provider.name] = provider.library_path\n",
    "            # DO NOT call provider.try_register in python. That will register to the native env.\n",
    "    return eps\n",
    "\n",
    "def _regsiter_executino_providers_to_onnxruntime_genai():\n",
    "    import onnxruntime_genai as og\n",
    "\n",
    "    paths = _get_ep_paths()\n",
    "    for item in paths.items():\n",
    "        print(f\"----register ort genai ep---- {item[0]} {item[1]}\")\n",
    "        og.register_execution_provider_library(item[0], item[1])\n",
    "\n",
    "_fix_winrt_runtime()\n",
    "_regsiter_executino_providers_to_onnxruntime_genai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def get_session_options(obj):\n",
    "    if type(obj) is dict:\n",
    "        for k, v in obj.items():\n",
    "            if k == \"session_options\":\n",
    "                yield v\n",
    "            else:\n",
    "                for x in get_session_options(v):\n",
    "                    yield x\n",
    "    elif type(obj) is list:\n",
    "        for v in obj:\n",
    "            for x in get_session_options(v):\n",
    "                yield x\n",
    "\n",
    "\n",
    "def remove_provider_options(model_path):\n",
    "    genai_config_path = Path(model_path) / \"genai_config.json\"\n",
    "    data = json.loads(genai_config_path.read_text())\n",
    "    for session_option in get_session_options(data):\n",
    "        if 'provider_options' in session_option:\n",
    "            session_option['provider_options'] = [{k: dict() for k in opts.keys()} for opts in session_option['provider_options']]\n",
    "\n",
    "    json.dump(data, genai_config_path.open(\"w\"), indent=4)\n",
    "\n",
    "if ExecutionProvider == \"QNNExecutionProvider\":\n",
    "    remove_provider_options(model_folder)\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model = og.Model(model_folder)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "# Set the max length to something sensible by default,\n",
    "# since otherwise it will be set to the entire context length\n",
    "search_options = {}\n",
    "search_options[\"max_length\"] = 200\n",
    "\n",
    "chat_template = \"<｜User｜>{input}<｜Assistant｜><think>\"\n",
    "\n",
    "# Generate prompt (prompt template + input)\n",
    "prompt = f\"{chat_template.format(input=text)}\"\n",
    "\n",
    "# Encode the prompt using the tokenizer\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "# Create params and generator\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(**search_options)\n",
    "generator = og.Generator(model, params)\n",
    "\n",
    "# Append input tokens to the generator\n",
    "generator.append_tokens(input_tokens)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Output: \", end=\"\", flush=True)\n",
    "\n",
    "token_times = []\n",
    "\n",
    "# Stream the output\n",
    "while not generator.is_done():\n",
    "    start_time = time.time()\n",
    "    generator.generate_next_token()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Record the time for this token generation\n",
    "    token_time = end_time - start_time\n",
    "    token_times.append(token_time)\n",
    "\n",
    "    new_token = generator.get_next_tokens()[0]\n",
    "    print(tokenizer_stream.decode(new_token), end=\"\", flush=True)\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculate and display timing statistics\n",
    "if token_times:\n",
    "    total_tokens = len(token_times)\n",
    "    avg_time = sum(token_times) / total_tokens\n",
    "    \n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Average time per token: {avg_time:.4f} seconds\")\n",
    "    print(f\"Tokens per second: {total_tokens / sum(token_times):.2f}\")\n",
    "\n",
    "del generator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
