{
    "input_model": {
        "type": "PytorchModel",
        "model_path": "openai/clip-vit-base-patch16",
        "generative": false,
        "io_config": {
            "input_names": [
                "input_ids",
                "attention_mask"
            ],
            "input_shapes": [
                [
                    1,
                    77
                ],
                [
                    1,
                    1,
                    77,
                    77
                ]
            ],
            "input_types": [
                "int32",
                "float32"
            ],
            "output_names": [
                "embeds"
            ],
            "dynamic_axes": {
                "input_ids": {
                    "0": "batch_size",
                    "1": "sequence_length"
                },
                "attention_mask": {
                    "0": "batch_size",
                    "2": "sequence_length",
                    "3": "sequence_length"
                },
                "embeds": {
                    "0": "batch_size"
                }
            }
        },
        "model_loader": "load_text_model",
        "model_script": "openai_clip.py",
        "script_dir": ".",
        "model_attributes": {
            "architectures": "CLIPModel",
            "model_type": "clip"
        }
    },
    "systems": {
        "host_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "QNNExecutionProvider"
                    ]
                }
            ]
        }
    },
    "host": "host_system",
    "target": "host_system",
    "evaluator": "common_evaluator",
    "evaluate_input_model": false,
    "log_to_file": false,
    "data_configs": [
        {
            "name": "calib_data",
            "type": "HuggingfaceContainer",
            "load_dataset_config": {
                "data_name": "nlphuji/flickr30k",
                "split": "test"
            },
            "pre_process_data_config": {
                "type": "tokenize_dataset",
                "model_name": "openai/clip-vit-base-patch16",
                "input_cols": [
                    "caption"
                ],
                "seq_length": 77,
                "max_samples": 12
            },
            "dataloader_config": {
                "batch_size": 1
            },
            "user_script": "openai_clip.py",
            "script_dir": "."
        },
        {
            "name": "eval_data",
            "type": "HuggingfaceContainer",
            "load_dataset_config": {
                "data_name": "nlphuji/flickr30k",
                "split": "test"
            },
            "pre_process_data_config": {
                "type": "tokenize_dataset",
                "model_name": "openai/clip-vit-base-patch16",
                "input_cols": [
                    "caption"
                ],
                "seq_length": 77,
                "max_samples": 100
            },
            "post_process_data_config": {
                "type": "embed_post_process"
            },
            "dataloader_config": {
                "batch_size": 1
            },
            "user_script": "openai_clip.py",
            "script_dir": "."
        }
    ],
    "evaluators": {
        "common_evaluator": {
            "metrics": [
                {
                    "name": "latency",
                    "type": "latency",
                    "data_config": "eval_data",
                    "sub_types": [
                        {
                            "name": "avg",
                            "priority": 1
                        },
                        {
                            "name": "p75"
                        },
                        {
                            "name": "p90"
                        }
                    ]
                }
            ]
        }
    },
    "passes": {
        "conversion": {
            "type": "OnnxConversion",
            "target_opset": 17,
            "use_dynamo_exporter": false,
            "save_as_external_data": true
        },
        "to_fixed_shape": {
            "type": "DynamicToFixedShape",
            "dim_param": [
                "batch_size",
                "sequence_length"
            ],
            "dim_value": [
                1,
                77
            ]
        },
        "transformer_optimizer": {
            "type": "orttransformersoptimization",
            "model_type": "clip",
            "opt_level": 1,
            "optimization_options": {
                "enable_gelu": true,
                "enable_bias_gelu": false,
                "enable_layer_norm": true,
                "enable_skip_layer_norm": false,
                "enable_bias_skip_layer_norm": false,
                "enable_attention": false
            }
        },
        "surgery": {
            "type": "GraphSurgeries",
            "surgeries": [
                {
                    "surgeon": "ReplaceAttentionMaskValue"
                },
                {
                    "surgeon": "PowReduceSumPowDiv2LpNorm"
                }
            ]
        },
        "quantization": {
            "type": "OnnxStaticQuantization",
            "data_config": "calib_data",
            "quant_preprocess": true,
            "activation_type": "uint16",
            "precision": "uint8",
            "save_as_external_data": true
        },
        "addmetadata": {
            "type": "VitisAIAddMetaData",
            "config_meta_data_keys": [
                "architectures",
                "model_type"
            ],
            "activation_type": "uint16",
            "weight_type": "uint8",
            "quant_type": "OnnxStaticQuantization"
        }
    },
    "cache_dir": "cache",
    "output_dir": "model/clip_text"
}