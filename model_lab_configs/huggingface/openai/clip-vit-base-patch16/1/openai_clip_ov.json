{
    "input_model": {
        "type": "HfModel",
        "model_path": "openai/clip-vit-base-patch16"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "device": "npu",
                    "execution_providers": [
                        "OpenVINOExecutionProvider"
                    ]
                }
            ]
        }
    },
    "data_configs": [
        {
            "name": "quantize_data_config",
            "user_script": "openai_clip_ov.py",
            "load_dataset_config": {
                "type": "conceptual_captions_dataset",
                "data_name": "google-research-datasets/conceptual_captions",
                "model_path": "openai/clip-vit-base-patch16"
            },
            "dataloader_config": {
                "batch_size": 1,
                "drop_last": true
            }
        },
        {
            "name": "metric_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": {
                "type": "clip_dataset",
                "model_name": "openai/clip-vit-base-patch16",
                "dataset_name": "nlphuji/flickr30k",
                "start": 10,
                "end": 20
            },
            "dataloader_config": { "type": "no_auto_batch_dataloader" },
            "post_process_data_config": { "type": "clip_post_process" }
        }
    ],
    "evaluators": {
        "common_evaluator": {
            "metrics": [
                {
                    "name": "latency",
                    "type": "latency",
                    "sub_types": [
                        { "name": "avg", "priority": 1, "metric_config": { "warmup_num": 20, "repeat_test_num": 100 } },
                        { "name": "p90", "metric_config": { "warmup_num": 20, "repeat_test_num": 100 } }
                    ]
                }
            ]
        }
    },
    "passes": {
        "optimum_convert": {
            "type": "OpenVINOOptimumConversion",
            "extra_args": {
                "device": "npu"
            }
        },
        "ov_quantize": {
            "type": "OpenVINOQuantization",
            "target_device": "npu",
            "data_config": "quantize_data_config",
            "model_type": "TRANSFORMER",
            "user_script": "openai_clip_ov.py",
            "transform_fn": "custom_transform_func",
            "extra_configs": [
                {
                    "advanced_quantization_parameters": {
                        "smooth_quant_alpha": 0.6
                    }
                }
            ]
        },
        "io_update": {
            "type": "OpenVINOIoUpdate",
            "input_shapes": [
                [
                    10,
                    77
                ],
                [
                    1,
                    3,
                    224,
                    224
                ],
                [
                    10,
                    77
                ]
            ],
            "static": true
        },
        "encapsulation": {
            "type": "OpenVINOEncapsulation",
            "target_device": "npu",
            "ov_version": "2025.1"
        }
    },
    "search_strategy": false,
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "evaluator": "common_evaluator",
    "evaluate_input_model": false,
    "output_dir": "model/clip_vit_base_patch16_context_ov_static"
}