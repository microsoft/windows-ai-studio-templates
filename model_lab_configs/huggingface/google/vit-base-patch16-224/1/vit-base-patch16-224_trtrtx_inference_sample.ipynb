{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./model/model.onnx\"\n",
    "\n",
    "ExecutionProvider=\"NvTensorRTRTXExecutionProvider\"\n",
    "if ExecutionProvider == \"OpenVINOExecutionProvider\":\n",
    "    onnx_model_path = \"./model/ov_model_st_quant.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_winrt_runtime():\n",
    "    \"\"\"This function removes the msvcp140.dll from the winrt-runtime package.\n",
    "    So it does not cause issues with other libraries.\n",
    "    \"\"\"\n",
    "    from importlib import metadata\n",
    "    from pathlib import Path\n",
    "    site_packages_path = Path(str(metadata.distribution('winrt-runtime').locate_file('')))\n",
    "    dll_path = site_packages_path / 'winrt' / 'msvcp140.dll'\n",
    "    if dll_path.exists():\n",
    "        dll_path.unlink()\n",
    "            \n",
    "def _get_ep_paths() -> dict[str, str]:\n",
    "    from winui3.microsoft.windows.applicationmodel.dynamicdependency.bootstrap import (\n",
    "        InitializeOptions,\n",
    "        initialize\n",
    "    )\n",
    "    import winui3.microsoft.windows.ai.machinelearning as winml\n",
    "    eps = {}\n",
    "    with initialize(options = InitializeOptions.ON_NO_MATCH_SHOW_UI):\n",
    "        catalog = winml.ExecutionProviderCatalog.get_default()\n",
    "        providers = catalog.find_all_providers()\n",
    "        for provider in providers:\n",
    "            provider.ensure_ready_async().get()\n",
    "            eps[provider.name] = provider.library_path\n",
    "            # DO NOT call provider.try_register in python. That will register to the native env.\n",
    "    return eps\n",
    "\n",
    "def _regsiter_executino_providers_to_onnxruntime():\n",
    "    import onnxruntime as ort\n",
    "\n",
    "    paths = _get_ep_paths()\n",
    "    for item in paths.items():\n",
    "        print(f\"----register ort ep---- {item[0]} {item[1]}\")\n",
    "        ort.register_execution_provider_library(item[0], item[1])\n",
    "\n",
    "_fix_winrt_runtime()\n",
    "_regsiter_executino_providers_to_onnxruntime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "def imageTransform(example):\n",
    "    example[\"image\"] = preprocess(example[\"image\"])\n",
    "    return example\n",
    "datasetStream = load_dataset(\"timm/mini-imagenet\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
    "iterable_dataset = iter(datasetStream)\n",
    "selected_samples = [next(iterable_dataset) for _ in range(num_samples)]\n",
    "selected_samples = list(map(imageTransform, selected_samples))\n",
    "\n",
    "def get_imagenet_label_map():\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    cache_file = Path(f\"../../cache/data/imagenet_class_index.json\")\n",
    "    if not cache_file.exists():\n",
    "        import requests        \n",
    "        imagenet_class_index_url = (\n",
    "            \"https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/imagenet_class_index.json\"\n",
    "        )\n",
    "        response = requests.get(imagenet_class_index_url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        content = response.json()\n",
    "        cache_file.parent.resolve().mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(content, f)\n",
    "    else:\n",
    "        with open(cache_file) as f:\n",
    "            content = json.loads(f.read())\n",
    "\n",
    "    return {v[0]: int(k) for k, v in content.items()}\n",
    "\n",
    "label_map = get_imagenet_label_map()\n",
    "label_names = datasetStream.features[\"label\"].names\n",
    "\n",
    "def mini_to_imagenet_label(mini_label):\n",
    "    class_name = label_names[mini_label]\n",
    "    return label_map[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model metrics\n",
    "\n",
    "def evaluate_torch(model, selected_samples, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for example in selected_samples:\n",
    "            image = example[\"image\"].unsqueeze(0).to(device)\n",
    "            label = torch.tensor(example[\"label\"]).to(device)\n",
    "            label = mini_to_imagenet_label(label.item())\n",
    "            \n",
    "            start_time = time.time()\n",
    "            output = model(image)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append((end_time - start_time))\n",
    "            pred = torch.argmax(output.logits, dim=1)\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_latency = np.mean(latencies)\n",
    "    return accuracy, avg_latency\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(device)\n",
    "accuracy, avg_latency = evaluate_torch(model, selected_samples, device)\n",
    "\n",
    "print(f\"Original Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Original Model Average Latency Per Image: {avg_latency * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized model metrics\n",
    "\n",
    "def evaluate_onnx(session, selected_samples):\n",
    "    correct, total = 0, 0\n",
    "    latencies = []\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    for example in selected_samples:\n",
    "        image = np.expand_dims(example[\"image\"], axis=0)\n",
    "        label = example[\"label\"]\n",
    "        label = mini_to_imagenet_label(label)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output = session.run([output_name], {input_name: image.astype(np.float16)})[0]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latencies.append((end_time - start_time))\n",
    "        pred = np.argmax(output, axis=1)[0]\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_latency = np.mean(latencies)\n",
    "    return accuracy, avg_latency\n",
    "\n",
    "def add_ep_for_device(session_options, ep_name, device_type, ep_options=None):\n",
    "    ep_devices = ort.get_ep_devices()\n",
    "    for ep_device in ep_devices:\n",
    "        if ep_device.ep_name == ep_name and ep_device.device.type == device_type:\n",
    "            print(f\"Adding {ep_name} for {device_type}\")\n",
    "            session_options.add_provider_for_devices([ep_device], {} if ep_options is None else ep_options)\n",
    "            break\n",
    "\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "\n",
    "add_ep_for_device(session_options, ExecutionProvider, ort.OrtHardwareDeviceType.NPU)\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    onnx_model_path, # a model wirh QNN EPContext nodes\n",
    "    sess_options=session_options,\n",
    ")\n",
    "\n",
    "accuracy, avg_latency = evaluate_onnx(session, selected_samples)\n",
    "\n",
    "print(f\"Quantized Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Quantized Model Average Latency Per Image: {avg_latency * 1000:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-WCR-win32-x64-3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
