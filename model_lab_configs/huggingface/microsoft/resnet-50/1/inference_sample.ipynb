{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./model/model.onnx\"\n",
    "\n",
    "ExecutionProvider=\"QNNExecutionProvider\"\n",
    "transpose = False\n",
    "if ExecutionProvider == \"OpenVINOExecutionProvider\":\n",
    "    onnx_model_path = \"./model/ov_model_st_quant.onnx\"\n",
    "elif ExecutionProvider == \"VitisAIExecutionProvider\":\n",
    "    transpose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "url = \"https://onnxruntime.ai/images/dog.jpeg\"\n",
    "response = requests.get(url)\n",
    "# Save the image to a file\n",
    "with open(\"dog.jpeg\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "img = Image.open(\"dog.jpeg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "\n",
    "image_file_path = \"dog.jpeg\"\n",
    "\n",
    "# Create ONNX runtime session\n",
    "def add_ep_for_device(session_options, ep_name, device_type, ep_options=None):\n",
    "    ep_devices = ort.get_ep_devices()\n",
    "    for ep_device in ep_devices:\n",
    "        if ep_device.ep_name == ep_name and ep_device.device.type == device_type:\n",
    "            print(f\"Adding {ep_name} for {device_type}\")\n",
    "            session_options.add_provider_for_devices([ep_device], {} if ep_options is None else ep_options)\n",
    "\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "\n",
    "add_ep_for_device(session_options, ExecutionProvider, ort.OrtHardwareDeviceType.NPU)\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    onnx_model_path, # a model wirh QNN EPContext nodes\n",
    "    sess_options=session_options,\n",
    ")\n",
    "\n",
    "print(\"Available providers:\", session.get_providers())\n",
    "print(\"Current provider:\", session.get_provider_options())\n",
    "\n",
    "# Read and preprocess image\n",
    "image = Image.open(image_file_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "if transpose:\n",
    "    input_tensor = input_tensor.permute(1, 2, 0)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Run inference\n",
    "ort_inputs = {session.get_inputs()[0].name: input_batch.numpy()}\n",
    "ort_outputs = session.run(None, ort_inputs)\n",
    "\n",
    "# Postprocess to get softmax vector\n",
    "output = ort_outputs[0]\n",
    "softmax = torch.nn.functional.softmax(torch.tensor(output), dim=1)\n",
    "\n",
    "# Extract top 10 predicted classes\n",
    "top10 = torch.topk(softmax, 10)\n",
    "\n",
    "# Get label mapping\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "labels = weights.meta[\"categories\"]\n",
    "\n",
    "# Print results to console\n",
    "print(\"Top 10 predictions for ResNet50 v2...\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(f\"Label: {labels[top10.indices[0][i]]}, Confidence: {top10.values[0][i].item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
