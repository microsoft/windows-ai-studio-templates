# Introcuction to Model Lab

Model Lab enables users to convert models to quantized version to run on NPU or GPU devices. The quantized version of models are smaller but with similar quanlity. By running on NPU or GPU, it will benefit from lower power consumption and lower latency compared with CPU or original model setup.

## Prerequisites

The Model Lab offers a range of different conversion workflows for different models. They could need different target devices so based on the workflow used, you need to have

- A Qualcomm NPU compatible device
- A AMD NPU compatible device
- A Nvidia GPU compatible device
