# Get started with adding new template

A detailed guide to add a new template for AITK Model Lab.

## Add the new model to the list

First, the model needs to be added to the `model_list.json` to be ready to be shown.

The format is like

```
{
    "displayName": "google/vit-base-patch16-224",
    "icon": "gemini",
    "modelLink": "https://huggingface.co/google/vit-base-patch16-224",
    "id": "huggingface/google/vit-base-patch16-224",
    "runtimes": [
        "QNN",
        "AMDNPU",
        "IntelNPU"
    ],
    "architecture": "Transformer",
    "status": "Ready",
    "version": 1
}
```

Explanation for each field:

- displayName: display name
- icon: one of the `IconEnum` in `scripts\sanitize.py`. To add a new icon, please submit issue.
- modelLink: the link in the Huggingface
- id: the path to find the template
- runtimes: indicate what is supported for searching
- architecture: for searching
- status: `Ready` to be shown up. `Hide` to hide it until release time is reached.
- version: this will be set by `scripts\sanitize.py` from the folder structure

## Create the folder and put templates here

Create new folder following the id plus the version number like `model_lab_configs\huggingface\google\vit-base-patch16-224\1` and then put your files here like

- vit-base-patch16-224_qdq_qnn.json: required. The olive config. For requirements, see [OliveJsonRequirements.md](./OliveJsonRequirements.md)
- vit-base-patch16-224.py: optional. User script if needed
- .gitignore: optional. If you want to bring your own, please make sure it convers `requirements.md`
- README.md: required
- requirements.txt: optional. If exist, it will be installed after runtime requirements
    + For better compatibility support, should include lib version
- inference_sample.ipynb: required. We should guide user how to use the model in WinML
    + This one will be used for olive json. If you want to provide a specific one, you could create one like `vit-base-patch16-224_qdq_qnn_inference_sample.ipynb`

## Create the model_project.config

This file tells the Model Lab which json should be treated as workflow and checked. It usually like this:

```
{
    "workflows": [
        {
            "name": "Convert to QNN",
            "file": "vit-base-patch16-224_qdq_qnn.json",
            "template": "huggingface/google/vit-base-patch16-224",
            "version": 1,
            "templateName": "vit-base-patch16-224_qdq_qnn"
        }
    ],
    "modelInfo": {
        "id": "huggingface/google/vit-base-patch16-224"
    }
}
```

Explanation for each field:

- name: display name
- file: the olive json config
- template / version / templateName: automatically set from file path
- modelInfo - id: automatically set. Used for the model card

## Create the *.json.config

To show the UX, you need to create `vit-base-patch16-224_qdq_qnn.json.config` and fill in parameters.
As a general guide, please refer to other json configs.
In the config, config with `"autoGenerated": true` is generated by `scripts\sanitize.py` and you don't need to fill it.

You need to organize sections by yourselves. Usually in the sequence of `Quantization` and `Evaluation`.

For example, this will show an integer control to set `data_configs[0].pre_process_data_config.size`:

```
{
    "name": "Quantization Dataset Size",
    "type": "int",
    "path": "data_configs[0].pre_process_data_config.size",
    "template": {
        "path": "data_configs[0].pre_process_data_config.size",
        "template": "QuantizationDatasetSize"
    }
}
```

In the config, if template is used, you need only to write values for template part (the others will be filled with template default):

- name / type: these are copied from template `QuantizationDatasetSize` in `parameter_template.json`
- path: it is copied from `template - path`

## Run sanitize.py and fix issues

After all files are setup, please run `scripts\santize.py` and fix issues.

## Run pack_to_extension.py to try E2E

After there is no issues, please run `scripts\pack_to_extension.py` to pack the current templates git repo into the extension and try the new template E2E.

You could also use `--restore` to restore the original one.

## Submit pr with relative info

After testing is done, please submit pr with changes and include the result `metrics.json`, so reviewers know that the model is running well.

